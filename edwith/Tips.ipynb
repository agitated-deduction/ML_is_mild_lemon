{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate\n",
    "\n",
    "### gradient  기울기\n",
    "\n",
    "기울기와 learning rate의 관계를 통해 최적의 값을 찾아낸다.<br>\n",
    "learning rate : hyper-parameter\n",
    "\n",
    "tensorflow 상에서 이 값을 넣어서 loss(cost)를 구한다.<br>\n",
    "어떤 optimizer로 할지 learning rate를 넣어준다.\n",
    "\n",
    "learning_rate는 쉽게 말해서 한번에 얼마나 이동할 지.<br>\n",
    "learning_rate<br>\n",
    "너무 크게 설정하면 발산할 수도 있음.<br>\n",
    "너무 작게 설정하면 시간이 오래 걸림.<br>\n",
    "일반적으로 많이 쓰는 값은 0.01, 3e-4(Adam optimizer) 등..\n",
    "\n",
    "learning rate를 조절 할 수도 있음 \"learning rate decay\" 기법?<br>\n",
    "시간을 단축하기 위해 처음에 크게 잡았다가 더 이상 코스트가 줄어들 지 않으면 다시 learning rate를 설정해주는거임.<br>\n",
    "* step decay\n",
    "* Exponential decay\n",
    "* 1/t decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#구현되어 있는것 사용\n",
    "learning_reate = tf.train.exponential_decay(starter_learning_rate, global_step, 1000, 0.96, staircase = True)\n",
    "\n",
    "#직접 구현\n",
    "def exponential_decay(epoch):\n",
    "    starter_rate = 0.01\n",
    "    k = 0.96\n",
    "    exp_rate = starter_rate* exp(-k*t) #t가 뭐지? 암튼 조금씩 숫자가 작아지는거같음\n",
    "    return exp_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "## 데이터 전처리\n",
    "\n",
    "데이터가 밀집되어 있으나 소수의 큰 데이터들로 인해 그래프가 요상해짐(noisy data)\n",
    "\n",
    "feature scaling<br>\n",
    "\n",
    "* 표준화 standardization (mean distance)<br>\n",
    "x_new = (x-뮤)/시그마<br>\n",
    "x를 평균으로 뺀 것을 표준편차로 나눈다.<br>\n",
    "평균과의 차이로 그래프 형성 가능\n",
    "\n",
    "* 정규화 normalization (0~1)<br>\n",
    "x_new = (x - x_min)/(x_max - x_min)<br>\n",
    "x값을 최솟값으로 뺀 것과 최대 최소 값의 차이로 나눈다.\n",
    "\n",
    "\n",
    "##### Noisy data 제거<br>\n",
    "Will you order a pizza...? -> You order pizza<br>\n",
    "얼굴 인식 프로그램에서 배경 헤어스타일 악세사리 등 날리고 얼굴만 뽑아내기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardization = (data - np.mean(data))/sqrt(np.sum((data - np.mean(data))^2)/np.count(data))\n",
    "\n",
    "normalixation = (data - np.min(data, 0))/(np.max(data, 0) - np.min(data, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "과적합<br>\n",
    "학습ㅇ 반복될수록 model이 맞춰져가면서 acc가 올라간다.<br>\n",
    "그러나 데이터 편향되어서 새로운 입력이 들어왔을때 잘못된 결과가 나올 수 있음<br>\n",
    "ex ) 강아지와 고양이 데이터를 구분하는데 검정색 강아지 데이터가 많으면 검정색은 모두 강아지라고 인식 할 수 있음.<br>\n",
    "\n",
    "* feature를 어떻게 정하느냐에 따라 overfitting에 대한 방법 해결 가능!<br>\n",
    "    * get more training data - 트레이닝 데이터를 늘린다.\n",
    "    * smaller set of feature - 기존의 2차원 공간을 차원을 줄임?????\n",
    "                         PCA라는게 있음\n",
    "    * add additional feature - feature수를 늘림? underfitting일 때? 얘기인듯하다.\n",
    "    \n",
    "    \n",
    "* Reqularization 정규화 add term loss\n",
    "![lambda](./img/lambda.png)\n",
    "\n",
    "특정값을 추가해서 정규화 할 수 있다.\n",
    "람다값을 준다. \n",
    "람다가 뭔데\n",
    "\n",
    "값자기 튀는 값이 있으면 그걸 음 정규화? 해준다.\n",
    "loss 값에 term을 줌?\n",
    "\n",
    "* dropout\n",
    "* batch normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = decomposition.PCA(n_components=3)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regularization\n",
    "L2_loss = tf.nn.l2_loss(w) # output = sum(t ** 2) / 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
