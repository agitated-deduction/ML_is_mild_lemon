{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WeightInit_Dropout_BatchNormalization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKPEYSu5afZG41gguEGxg8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agitated-deduction/ML_is_mild_lemon/blob/master/edwith/WeightInit_Dropout_BatchNormalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aguaE1SvKhIf",
        "colab_type": "text"
      },
      "source": [
        "# Weight Initialization\n",
        "\n",
        "### xavier initialization (glorot initialization)\n",
        "\n",
        "시작점을 잘못 잡으면 local minima나 saddle point에서 학습을 멈출 수 있음.\n",
        "\n",
        "분산 Varience = 2/(channel_in+channel_out)\n",
        "\n",
        "위의 값을 분산으로 이용하는 어떤 랜덤한 분포로 weight를 설정한다.\n",
        "\n",
        "relu = He initialization은 relu함수에 특화된 초기화법\n",
        "\n",
        "he initialization이 뭐지? 평균은 0인데 분산은 xavier보다 훨씬 크게?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_LxucBJKmMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_init_random = tf.keras.initializers.RandomNormal()\n",
        "weight_init_xavier = tf.keras.initializers.glorot_uniform()\n",
        "weight_init_he = tf.keras.initializers.he_uniform()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlOeH_-wNFmn",
        "colab_type": "text"
      },
      "source": [
        "# Dropout\n",
        "\n",
        "#### over-fitting이나 under-fitting이 되지 않고 잘 fitting될 수 있도록 도와주는 regularization.\n",
        "이외에 L2 regularization, batch normalization, 혹은 데이터 개수를 늘리는 방식으로 fitting이 잘 되도록 도울 수 있음\n",
        "\n",
        "dropout은 각각의 뉴런들 중 몇 개의 뉴런을 끄고 학습하는 것을 의미한다.  \n",
        "모든 뉴런/노드를 이용해서 학습하는 것이 아니라 일부분으로 학습을 한다. 노드는 랜덤으로 끈다.\n",
        "테스트할 때 모든 노드를 활용해 테스트를 한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlFcoBnmND_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def dropout(rate):\n",
        "   return tf.keras.layers.Dropout(rate) #rate는 0과 1 사이. 몇퍼센트의 뉴런을 끌 것인가.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdlvdHzicBoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in ragne(2):\n",
        "  model.add(dense(256, weight_init))\n",
        "  model.add(relu())\n",
        "  model.add(dropout(rate = 0.5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uwv15jzcfsC",
        "colab_type": "text"
      },
      "source": [
        "### dropout은 학습할때는 일부의 노드 사용하고, 테스트 할때는 모든 노드 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7TJxyh7cEiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "logits = model(images, training = True) #이런 식으로 training값을 주면 됨.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujOCjeLbcjoB",
        "colab_type": "text"
      },
      "source": [
        "training이 true이면 dropout을 사용한다는 뜻.\n",
        "loss_fn에서는 training = True\n",
        "accuracy_fn에서는 training = False로 주면 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW_Z-Y_BdB24",
        "colab_type": "text"
      },
      "source": [
        "#Batch Normalization\n",
        "\n",
        "Internal Covariate Shift(분포가 이상하게 바뀜) 이걸 막기 위한게 batch normalizetion인데\n",
        "레이어의 인풋으로 들어오는 distribution을 계속 Normalization 시켜줘서 distribution을 일정하게 유지시키자!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_uOlX3Vcx1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_norm():\n",
        "  return tf.keras.layers.BatchNormalization()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx5EY1APIwNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in ragne(2):\n",
        "  model.add(dense(256, weight_init))\n",
        "  model.add(batch_norm())\n",
        "  model.add(relu())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0GKnry7ZGgG",
        "colab_type": "text"
      },
      "source": [
        "layer -> norm -> activation (가장 많이 사용)<br>\n",
        "norm -> activation -> layer<br>\n",
        "의 순서로 많이 쓴다.\n",
        "\n"
      ]
    }
  ]
}